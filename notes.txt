Need:
Reward Function R(s): probably 1 stroke
Transition Function: defines a transition T where being in state S and taking an action ‘a’ takes us to state S’ - given in text file
    Probability Function: P(S'|s,a) - given in text file
Utility Function = R(s) + max( discount*( (prob * each option) ...+... () ) )

Model-Based
    Keep track of how many times you arrive at state s' when taking action a from state s --> Transition Probability P(s'|s,a)
    Keep track of reward function as well

Model-Free
    Estimate the utility - I think


As you make the discount value closer to 1 - Each utility value for the policy becomes more and more skewed
    There is a more noticeable difference between the utility of each state causing the policy to be consistently the same

As you make the discount value closer to 0 - The difference between each utility value decreases
    Since the values are closer to each other, the policy is more likely to change

I decided to stop learning when the utility values reach convergence

With a higher exploration value, the utility values will be more varied each time learning is done
    Whereas with a smaller exploration value there will not be much variation